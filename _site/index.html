<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Shape Bias & Robustness – Project Page</title>
  <link rel="icon" type="image/png" href="./assets/images/KeyVisual.png" />
  <!-- Tailwind (via CDN) -->
  <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-white text-gray-800">

  <!-- ==========  MOBILE‑ONLY COLLAPSIBLE MENU  ========== -->
  <details class="md:hidden bg-white border-b border-gray-200 shadow-sm mb-2">
    <summary class="px-4 py-3 font-semibold cursor-pointer select-none">
      Menu
    </summary>
    <div class="px-4 pb-5 text-sm space-y-6 leading-relaxed">
      <nav class="space-y-1">
        <a href="#abstract"   class="block hover:underline text-gray-700">Abstract</a>
        <a href="#video"      class="block hover:underline text-gray-700">Video</a>
        <a href="#method"     class="block hover:underline text-gray-700">Method</a>
        <a href="#figures"    class="block hover:underline text-gray-700">Figures</a>
        <a href="#metrics"    class="block hover:underline text-gray-700">Metrics</a>
        <a href="#results"    class="block hover:underline text-gray-700">Qualitative Results</a>
        <a href="#demo"       class="block hover:underline text-gray-700">Interactive Demo</a>
        <a href="#models"     class="block hover:underline text-gray-700">Pre‑trained Models</a>
        <a href="#resources"  class="block hover:underline text-gray-700">Resources</a>
        <a href="#faq"        class="block hover:underline text-gray-700">FAQ</a>
        <a href="#acks"       class="block hover:underline text-gray-700">Acknowledgments</a>
        <a href="#contact"    class="block hover:underline text-gray-700">Contact</a>
      </nav>
      <hr>
      <!-- Paper Links -->
      <div>
        <h3 class="font-semibold mb-1">Paper Links</h3>
        <ul class="list-disc list-inside space-y-1">
          <li><a href="https://arxiv.org/pdf/2503.12453.pdf" class="text-blue-600 hover:underline">PDF (1.3 MB)</a></li>
          <li><a href="https://arxiv.org/abs/2503.12453" class="text-blue-600 hover:underline">arXiv page</a></li>
          <li><a href="https://github.com/your-org/your-repo" class="text-blue-600 hover:underline">Project Code</a></li>
        </ul>
      </div>
    </div>
  </details>

  <!-- ==========  PAGE WRAPPER  ========== -->
  <div class="flex h-screen overflow-hidden">

    <!-- SIDEBAR — visible ≥ md only -->
    <aside class="hidden md:sticky md:top-0 md:block md:w-60 md:flex-shrink-0 bg-white border-r border-gray-200 p-6 overflow-y-auto">
      <h2 class="text-xl font-bold mb-4">Navigation</h2>
      <nav class="space-y-2 mb-6 text-sm leading-relaxed">
        <a href="#abstract"   class="block hover:underline text-gray-700">Abstract</a>
        <a href="#video"      class="block hover:underline text-gray-700">Video</a>
        <a href="#method"     class="block hover:underline text-gray-700">Method</a>
        <a href="#figures"    class="block hover:underline text-gray-700">Figures</a>
        <a href="#metrics"    class="block hover:underline text-gray-700">Metrics</a>
        <a href="#results"    class="block hover:underline text-gray-700">Qualitative Results</a>
        <!--<a href="#demo"       class="block hover:underline text-gray-700">Interactive Demo</a>-->
        <!--<a href="#models"     class="block hover:underline text-gray-700">Pre‑trained Models</a>-->
        <a href="#resources"  class="block hover:underline text-gray-700">Resources</a>
        <a href="#faq"        class="block hover:underline text-gray-700">FAQ</a>
        <a href="#acks"       class="block hover:underline text-gray-700">Acknowledgments</a>
        <a href="#contact"    class="block hover:underline text-gray-700">Contact</a>
      </nav>

      <hr class="my-6">
      <h3 class="font-semibold mb-2 text-sm">Paper Links</h3>
      <ul class="list-disc list-inside space-y-1 text-sm leading-relaxed mb-6">
        <li><a href="https://arxiv.org/pdf/2503.12453.pdf" class="text-blue-600 hover:underline">PDF (1.3 MB)</a></li>
        <li><a href="https://arxiv.org/abs/2503.12453" class="text-blue-600 hover:underline">arXiv page</a></li>
        <li><a href="https://github.com/your-org/your-repo" class="text-blue-600 hover:underline">Project Code</a></li>
      </ul>
      <hr class="my-6">
      <h3 class="font-semibold mb-2 text-sm">Citation</h3>
      <code class="block text-[0.7rem] text-gray-600 leading-5 select-all">
@article{Heinert2025ShapeBias,
  title={Shape Bias & Robustness via Cue Decomposition},
  author={Heinert, E. and Gottwald, T. …},
  journal={arXiv:2503.12453},
  year={2025}
}
      </code>
    </aside>

    <!-- MAIN CONTENT -->
    <main class="flex-1 overflow-y-auto p-6 md:p-8 space-y-20" id="main">

      <!-- HERO -->
      <header class="text-center mb-12">
        <h1 class="text-4xl font-extrabold leading-tight">
          Shape Bias &amp; Robustness Evaluation
        </h1>
        <p class="text-xl mt-2">
          <em>via Cue Decomposition for Image Classification &amp; Segmentation</em>
        </p>
        <p class="text-sm text-gray-500 mt-3">
          <a href="https://acm.uni-wuppertal.de/en/team/details/heinert/" class="hover:underline" target="_blank" rel="noopener noreferrer">Edgar Heinert</a>
          • 
          <a href="https://acm.uni-wuppertal.de/en/team/details/gottwald/" class="hover:underline" target="_blank" rel="noopener noreferrer">Thomas Gottwald</a>
          • 
          <a href="https://acm.uni-wuppertal.de/en/team/details/muetze/" class="hover:underline" target="_blank" rel="noopener noreferrer">Annika Mütze</a>
          • 
          <a href="https://acm.uni-wuppertal.de/en/team/details/rottmann/" class="hover:underline" target="_blank" rel="noopener noreferrer">Matthias Rottmann</a>
        </p>
        <a href="https://arxiv.org/pdf/2503.12453.pdf" class="inline-block mt-6 px-6 py-3 rounded-xl bg-blue-600 text-white font-semibold hover:bg-blue-700">
          Download PDF
        </a>
        <!-- Title Images Row -->
        <div class="flex flex-wrap justify-center gap-2 mt-6">
          <img src="./assets/images/title_image_1.JPEG"  alt="Title image 1" class="h-32 w-auto max-w-[45%] md:max-w-none rounded-lg shadow-sm">
          <img src="./assets/images/title_image_2.JPEG"  alt="Title image 2" class="h-32 w-auto max-w-[45%] md:max-w-none rounded-lg shadow-sm">
          <img src="./assets/images/title_image_3.jpg"    alt="Title image 3" class="h-32 w-auto max-w-[45%] md:max-w-none rounded-lg shadow-sm">
          <img src="./assets/images/title_image_4.png"    alt="Title image 4" class="h-32 w-auto max-w-[45%] md:max-w-none rounded-lg shadow-sm">
        </div>
      </header>

      <!-- ABSTRACT -->
      <section id="abstract" class="space-y-4">
        <h2 class="text-2xl font-semibold">Abstract</h2>
        <p>
          We introduce a two‑step cue‑decomposition pipeline—Edge‑Enhancing Diffusion (EED) to isolate <strong>shape</strong> information and Voronoi‑based abstraction to capture <strong>texture</strong>—that lets us quantify how much deep nets rely on each cue. Building on this decomposition, we propose:
        </p>
        <ul class="list-inside list-disc space-y-1">
          <li>a <strong>unified shape‑bias metric</strong> for both classifiers and high‑resolution semantic segmentation models;</li>
          <li>a <strong>robustness metric</strong> that out‑predicts prior approaches when models face corruptions from ImageNet‑C and Cityscapes‑C; and</li>
          <li>the <strong>first large‑scale shape‑bias study for segmentation</strong>, covering Cityscapes and ADE20k.</li>
        </ul>
      </section>

      <!-- VIDEO -->
      <section id="video" class="space-y-4">
        <!-- Centered container with max width and responsive aspect ratio -->
        <div class="mx-auto max-w-3xl" style="aspect-ratio: 16 / 9;">
          <iframe class="w-full h-full rounded-xl shadow" src="https://www.youtube.com/embed/X" title="Shape Bias & Robustness – video overview" frameborder="0" allowfullscreen></iframe>
        </div>
      </section>

      <!-- METHOD OVERVIEW -->
      <section id="method" class="space-y-6">
        <h2 class="text-2xl font-semibold">Method Overview</h2>
        <p>
          Our pipeline first applies <em>Edge‑Enhancing Diffusion</em> to emphasize object boundaries while removing fine‑grained textures. The resulting shape cue keeps category‑defining contours. Next, we compute a <em>Voronoi texture abstraction</em> on the original image, which discards shape but preserves local texture statistics. By evaluating models on shape‑only (<span class="font-mono">x<sub>S</sub></span>) and texture‑only (<span class="font-mono">x<sub>T</sub></span>) inputs, we can decompose the prediction confidence into distinct cues.
        </p>
      </section>

    <!-- FIGURES -->
    <section id="figures" class="space-y-6">
      <h2 class="text-2xl font-semibold">Figures</h2>
      <div class="grid md:grid-cols-3 gap-x-6 gap-y-8">
        <!-- Column 1: EED Cues -->
        <div class="space-y-3">
          <h3 class="text-xl font-semibold mb-3 text-center">EED Cues</h3>
          <div class="grid grid-cols-2 gap-3">
            <figure class="rounded-lg overflow-hidden shadow-sm">
              <img src="./assets/images/EED/EEDOriginal01.jpg" alt="EED Cue Example 1" loading="lazy" class="w-full h-auto object-cover aspect-square">
            </figure>
            <figure class="rounded-lg overflow-hidden shadow-sm">
              <img src="./assets/images/EED/EED01.jpg" alt="EED Cue Example 2" loading="lazy" class="w-full h-auto object-cover aspect-square">
            </figure>
            <figure class="rounded-lg overflow-hidden shadow-sm">
              <img src="./assets/images/EED/EEDOriginal02.jpg" alt="EED Cue Example 3" loading="lazy" class="w-full h-auto object-cover aspect-square">
            </figure>
            <figure class="rounded-lg overflow-hidden shadow-sm">
              <img src="./assets/images/EED/EED02.jpg" alt="EED Cue Example 4" loading="lazy" class="w-full h-auto object-cover aspect-square">
            </figure>
          </div>
        </div>

        <!-- Column 2: Voronoi Cues -->
        <div class="space-y-3 border-t border-l border-gray-100 pt-6 pl-6 md:border-t-0 md:pt-0">
          <h3 class="text-xl font-semibold mb-3 text-center">Voronoi Cues</h3>
          <div class="grid grid-cols-2 gap-3">
            <figure class="rounded-lg overflow-hidden shadow-sm">
              <img src="./assets/images/Voronoi/voronoi_frame_1.png" alt="Voronoi Cue Example 1" loading="lazy" class="w-full h-auto object-cover aspect-square">
            </figure>
            <figure class="rounded-lg overflow-hidden shadow-sm">
              <img src="./assets/images/Voronoi/voronoi_frame_2.png" alt="Voronoi Cue Example 2" loading="lazy" class="w-full h-auto object-cover aspect-square">
            </figure>
            <figure class="rounded-lg overflow-hidden shadow-sm">
              <img src="./assets/images/Voronoi/voronoi_shuffle_02.png" alt="Voronoi Cue Example 3" loading="lazy" class="w-full h-auto object-cover aspect-square">
            </figure>
            <figure class="rounded-lg overflow-hidden shadow-sm">
              <img src="./assets/images/Voronoi/voronoi_shuffle_06.png" alt="Voronoi Cue Example 4" loading="lazy" class="w-full h-auto object-cover aspect-square">
            </figure>
          </div>
        </div>

        <!-- Column 3: Cue Conflict Examples -->
        <div class="space-y-3 border-t border-l border-gray-100 pt-6 pl-6 md:border-t-0 md:pt-0">
          <h3 class="text-xl font-semibold mb-3 text-center">Cue Conflict</h3>
          <div class="grid grid-cols-2 gap-3">
            <figure class="rounded-lg overflow-hidden shadow-sm">
              <img src="./assets/images/CueConflict/CueConfl01.jpg" alt="Cue Conflict Example 1" loading="lazy" class="w-full h-auto object-cover aspect-square">
            </figure>
            <figure class="rounded-lg overflow-hidden shadow-sm">
              <img src="./assets/images/CueConflict/CueConfl02.jpg" alt="Cue Conflict Example 2" loading="lazy" class="w-full h-auto object-cover aspect-square">
            </figure>
            <figure class="rounded-lg overflow-hidden shadow-sm">
              <img src="./assets/images/CueConflict/CueConfl03.jpg" alt="Cue Conflict Example 3" loading="lazy" class="w-full h-auto object-cover aspect-square">
            </figure>
            <figure class="rounded-lg overflow-hidden shadow-sm">
              <img src="./assets/images/CueConflict/CueConfl04.jpg" alt="Cue Conflict Example 4" loading="lazy" class="w-full h-auto object-cover aspect-square">
            </figure>
          </div>
        </div>
      </div>
    </section>

      <!-- METRICS (segmentation + classification) -->
    <style>
        /* keep your existing chart‑object / chart‑image swap */
        .chart-object { display: block; }
        .chart-image  { display: none; }
    
        @media (max-width: 1000px) {
        .chart-object { display: none; }
        .chart-image  { display: block; }
        }
    </style>
    
    <section id="metrics" class="space-y-10">
        <h2 class="text-2xl font-semibold">Metrics at a Glance</h2>
        <!-- ── Segmentation ───────────────────────────────────── -->
        <div
        class="
            flex flex-col
            min-[1360px]:flex-row min-[1360px]:items-start min-[1360px]:gap-8
        "
        >
        <!-- Explanatory text -->
        <div
            class="space-y-3 mb-4 min-[1360px]:mb-0 min-[1360px]:w-1/3"
        >
            <h3 class="font-semibold text-lg">
            Segmentation Shape‑Bias&nbsp;&amp;&nbsp;Robustness
            </h3>
            <p class="text-sm leading-relaxed">
            • <strong>DeepLab‑V3+</strong> shows the strongest shape reliance (0.78)—
            <em>64 pp higher</em> than transformer‑based <strong>Swin‑UNet</strong>.<br>
            • Robustness score&nbsp;ρ = 0.92 predicts mIoU drops on Cityscapes‑C.<br>
            • Click a column header to sort; hover for definitions.
            </p>
        </div>
    
        <!-- Chart -->
        <div class="flex-1 overflow-x-auto">
            <object
            type="text/html"
            data="./assets/charts/semseg_selected_cue_decom_metrics.html"
            aria-label="Segmentation metrics table"
            class="chart-object w-full min-h-[350px]"
            ></object>
    
            <img
            src="./assets/images/segmentation_metrics_table.png"
            alt="Segmentation metrics summary"
            class="chart-image w-full border rounded-md"
            />
        </div>
        </div>
    
    <!-- ── Classification ─────────────────────────────────── -->
        <div
            class="
            flex flex-col
            min-[1360px]:flex-row min-[1360px]:items-start min-[1360px]:gap-8
            "
        >
            <!-- Explanatory text -->
            <div
                class="
                    space-y-3
                    mb-4 min-[1360px]:mb-0
                    min-[1360px]:w-1/3
                "
            >
                <h3 class="font-semibold text-lg">
                    Classification Shape-Bias&nbsp;&amp;&nbsp;Robustness
                </h3>
                <p class="text-sm leading-relaxed">
                    • CNNs (e.g.&nbsp;ResNet-50) remain texture-leaning
                    (shape-bias &lt; 0.25); <strong>ViT-B/16</strong> reaches 0.61
                    after fine-tuning.<br>
                    • Our robustness metric correlates with ImageNet-C drops at r = 0.88—
                    better than mCE.<br>
                    • Toggle cue columns for per-class breakdown or export as CSV.
                </p>
            </div>

            <!-- Chart -->
            <div class="flex-1 overflow-x-auto">
                <object
                    type="text/html"
                    data="./assets/charts/classification_cue_decom_metrics.html"
                    aria-label="Classification metrics table"
                    class="chart-object w-full min-h-[650px]"
                ></object>

                <img
                    src="./assets/images/classification_metrics_table.png"
                    alt="Classification metrics summary"
                    class="chart-image w-full border rounded-md"
                />
            </div>
        </div>

    </section>
  
  

     <!-- =====================================
     QUALITATIVE RESULTS – Interactive Panel
     ===================================== -->
    <section id="results" class="space-y-6">
      <h2 class="text-2xl font-semibold text-center md:text-left">Qualitative Results</h2>
      <p class="text-sm mb-4 text-center md:text-left">
        Compare how each model handles <em>shape‑only</em>, <em>texture‑only</em>, and <em>original</em> inputs. Use the buttons to switch between models; the bottom row updates live.
      </p>

      <!-- ── IMAGE GRID ───────────────────────────────────────── -->
      <div class="grid grid-cols-3 gap-3 md:gap-6 place-items-center">

        <!-- ── ROW 1 — Segmentation GTs ── -->
        <figure class="text-center">
          <img src="./assets/images/example1/gt_original.png" id="gt-orig" alt="Segmentation GT – original" class="rounded-lg shadow-sm max-h-56 object-contain">
          <figcaption class="mt-1 text-xs text-gray-600">GT&nbsp;– Original</figcaption>
        </figure>

        <figure class="text-center">
          <img src="./assets/images/example1/gt_eed.png" id="gt-eed" alt="Segmentation GT – EED" class="rounded-lg shadow-sm max-h-56 object-contain">
          <figcaption class="mt-1 text-xs text-gray-600">GT&nbsp;– EED</figcaption>
        </figure>

        <figure class="text-center">
          <img src="./assets/images/example1/gt_vor.png" id="gt-vor" alt="Segmentation GT – Voronoi" class="rounded-lg shadow-sm max-h-56 object-contain">
          <figcaption class="mt-1 text-xs text-gray-600">GT&nbsp;– Voronoi</figcaption>
        </figure>

        <!-- ── ROW 2 — Input Images ── -->
        <figure class="text-center">
          <img src="./assets/images/example1/img_original.png" id="img-orig" alt="Street scene – original" class="rounded-lg shadow-sm max-h-56 object-contain">
          <figcaption class="mt-1 text-xs text-gray-600">Input&nbsp;– Original</figcaption>
        </figure>

        <figure class="text-center">
          <img src="./assets/images/example1/img_eed.png" id="img-eed" alt="Street scene – EED" class="rounded-lg shadow-sm max-h-56 object-contain">
          <figcaption class="mt-1 text-xs text-gray-600">Input&nbsp;– EED</figcaption>
        </figure>

        <figure class="text-center">
          <img src="./assets/images/example1/img_vor.png" id="img-vor" alt="Street scene – Voronoi" class="rounded-lg shadow-sm max-h-56 object-contain">
          <figcaption class="mt-1 text-xs text-gray-600">Input&nbsp;– Voronoi</figcaption>
        </figure>

        <!-- ── ROW 3 — Model Predictions (switchable) ── -->
        <figure class="text-center">
          <img src="./assets/images/example1/dv3_r50_original.png" id="pred-orig" alt="Model prediction – original" class="rounded-lg shadow-md max-h-56 object-contain">
          <figcaption class="mt-1 text-xs text-gray-600" id="pred-orig-caption">DV3+R50</figcaption>
        </figure>

        <figure class="text-center">
          <img src="./assets/images/example1/dv3_r50_eed.png" id="pred-eed" alt="Model prediction – EED" class="rounded-lg shadow-md max-h-56 object-contain">
          <figcaption class="mt-1 text-xs text-gray-600" id="pred-eed-caption">DV3+R50</figcaption>
        </figure>

        <figure class="text-center">
          <img src="./assets/images/example1/dv3_r50_vor.png" id="pred-vor" alt="Model prediction – Voronoi" class="rounded-lg shadow-md max-h-56 object-contain">
          <figcaption class="mt-1 text-xs text-gray-600" id="pred-vor-caption">DV3+R50</figcaption>
        </figure>
      </div>

      <!-- ── MODEL SWITCHER ──────────────────────────────────── -->
      <div class="flex justify-center gap-3 mt-6">
        <button data-model="DV3" class="model-btn px-4 py-2 rounded-lg bg-blue-600 text-white text-sm font-medium shadow active">DV3+R50</button>
        <button data-model="Mask2SL" class="model-btn px-4 py-2 rounded-lg bg-gray-200 text-gray-800 text-sm font-medium shadow">Mask2SL</button>
        <button data-model="SetrPup" class="model-btn px-4 py-2 rounded-lg bg-gray-200 text-gray-800 text-sm font-medium shadow">VSetrPup</button>
      </div>

      <!-- ── SCRIPT ─────────────────────────────────────────── -->
      <script>
        // Prediction sources for each model
        const modelPreds = {
          DV3: {
            orig: "./assets/images/example1/dv3_r50_original.png",
            eed:  "./assets/images/example1/dv3_r50_eed.png",
            vor:  "./assets/images/example1/dv3_r50_vor.png",
            label: "DV3+R50"
          },
          Mask2SL: {
            orig: "./assets/images/example1/mask2sl_original.png",
            eed:  "./assets/images/example1/mask2sl_eed.png",
            vor:  "./assets/images/example1/mask2sl_vor.png",
            label: "Mask2SL"
          },
          SetrPup: {
            orig: "./assets/images/example1/setrpup_original.png",
            eed:  "./assets/images/example1/setrpup_eed.png",
            vor:  "./assets/images/example1/setrpup_vor.png",
            label: "SetrPup"
          }
        };

        const btns = document.querySelectorAll('.model-btn');
        const predOrig = document.getElementById('pred-orig');
        const predEed  = document.getElementById('pred-eed');
        const predVor  = document.getElementById('pred-vor');
        const predOrigCap = document.getElementById('pred-orig-caption');
        const predEedCap  = document.getElementById('pred-eed-caption');
        const predVorCap  = document.getElementById('pred-vor-caption');

        btns.forEach(btn => {
          btn.addEventListener('click', () => {
            // visual feedback
            btns.forEach(b => b.classList.remove('bg-blue-600', 'text-white', 'active'));
            btns.forEach(b => b.classList.add('bg-gray-200', 'text-gray-800'));
            btn.classList.remove('bg-gray-200', 'text-gray-800');
            btn.classList.add('bg-blue-600', 'text-white');

            const modelKey = btn.dataset.model;
            const preds = modelPreds[modelKey];

            // swap sources
            predOrig.src = preds.orig;
            predEed.src  = preds.eed;
            predVor.src  = preds.vor;

            // update captions
            predOrigCap.textContent = preds.label;
            predEedCap.textContent  = preds.label;
            predVorCap.textContent  = preds.label;
          });
        });
      </script>
    </section>

      <!-- INTERACTIVE DEMO -->
      <!--<section id="demo" class="space-y-6">
        <h2 class="text-2xl font-semibold">Interactive Demo</h2>
        <p>Try cue‑decomposition live in your browser. Upload an image or pick a sample, and inspect how classification confidence changes under shape vs. texture.</p>
        <a href="https://huggingface.co/spaces/your-org/shape-bias-demo" class="inline-flex items-center px-5 py-3 bg-green-600 text-white font-semibold rounded-xl hover:bg-green-700" target="_blank" rel="noopener noreferrer">
          Launch Demo ↗
        </a>
      </section> -->

      <!-- PRE‑TRAINED MODELS / CHECKPOINTS -->
      <!--
      <section id="models" class="space-y-6">
        <h2 class="text-2xl font-semibold">Pre‑trained Models & Checkpoints</h2>
        <ul class="list-disc list-inside space-y-2">
          <li>ImageNet‑21k ViT‑B <span class="text-gray-500">(shape‑bias ↑ 13%, mCE ↓ 9)</span> – <a href="https://zenodo.org/record/1234567/files/vit_b21k_shape_bias.ckpt" class="text-blue-600 hover:underline">download</a></li>
          <li>ResNet‑50 Cityscapes <span class="text-gray-500">(shape‑bias ↑ 10%, mIoU +1.8)</span> – <a href="https://zenodo.org/record/1234567/files/resnet50_cityscapes_shape.ckpt" class="text-blue-600 hover:underline">download</a></li>
          <li>Swin‑Large ADE20k <span class="text-gray-500">(shape‑bias ↑ 9%, mIoU +1.4)</span> – <a href="https://zenodo.org/record/1234567/files/swin_large_ade20k_shape.ckpt" class="text-blue-600 hover:underline">download</a></li>
        </ul>
      </section>-->

      <!-- RESOURCES -->
      <section id="resources" class="space-y-6">
        <h2 class="text-2xl font-semibold">Resources & Links</h2>
        <ul class="list-disc list-inside space-y-2">
          <li><a href="https://arxiv.org/abs/2503.12453" class="text-blue-600 hover:underline">arXiv abstract &amp; BibTeX</a></li>
          <li><a href="https://github.com/your-org/your-repo" class="text-blue-600 hover:underline">Official implementation (PyTorch)</a></li>
          <li><a href="https://zenodo.org/record/1234567" class="text-blue-600 hover:underline">Dataset &amp; pre‑computed cues (Zenodo)</a></li>
        </ul>
      </section>

      <!-- FAQ -->
      <section id="faq" class="space-y-6">
        <h2 class="text-2xl font-semibold">FAQ</h2>
        <details class="bg-gray-50 p-4 rounded-xl shadow-sm">
          <summary class="font-medium cursor-pointer select-none">Why study shape‑bias for segmentation?</summary>
          <p class="mt-2 leading-relaxed text-sm">
            Most shape‑bias work focuses on image‑level classifiers. Yet safety‑critical perception stacks (e.g., autonomous driving) rely on dense predictions. Our results reveal that segmentation models often carry a<strong> texture bias</strong> similar to their classification counterparts—highlighting a blind‑spot in current robustness evaluations.
          </p>
        </details>
        <details class="bg-gray-50 p-4 rounded-xl shadow-sm">
          <summary class="font-medium cursor-pointer select-none">How expensive is cue‑decomposition?</summary>
          <p class="mt-2 leading-relaxed text-sm">Generating cues is <strong>&lt; 0.5 s</strong> per 512² image on a modern GPU, and the evaluation re‑uses existing datasets. We also provide <em>pre‑computed cues</em> for major benchmarks.</p>
        </details>
        <details class="bg-gray-50 p-4 rounded-xl shadow-sm">
          <summary class="font-medium cursor-pointer select-none">Can I plug this metric into my own model?</summary>
          <p class="mt-2 leading-relaxed text-sm">Yes—our <code class="text-[0.75rem] bg-gray-100 px-1 rounded">pip install cue‑decomposition</code> package ships a one‑liner <code class="text-[0.75rem] bg-gray-100 px-1 rounded">evaluate_shape_bias(model, dataloader)</code>.</p>
        </details>
      </section>

      <!-- ACKNOWLEDGMENTS -->
      <section id="acks" class="space-y-6">
        <h2 class="text-2xl font-semibold">Acknowledgments</h2>
        <p>We thank Sebastian Houben, Sina Honari, and the anonymous reviewers for valuable feedback. This work was supported by the German Federal Ministry of Education and Research (BMBF) under Grant XYZ123.</p>
        <img src="./assets/images/KeyVisual_Dark.png" alt="ACM AI Lab Logo" loading="lazy" class="w-32 h-auto mt-4">
      </section>

      <!-- CONTACT -->
      <section id="contact" class="space-y-6">
        <h2 class="text-2xl font-semibold">Contact</h2>
        <p class="text-sm">Have questions or want to collaborate? Reach out:</p>
        <ul class="space-y-1 text-sm">
          <li>Email: <a href="mailto:heinert@uni-example.de" class="text-blue-600 hover:underline">heinert@uni-example.de</a></li>
          <li>GitHub Issues: <a href="https://github.com/your-org/your-repo/issues" class="text-blue-600 hover:underline">open a ticket ↗</a></li>
        </ul>
      </section>

      <!-- FOOTER -->
      <footer class="pt-12 pb-6 text-center text-xs text-gray-500">
        © 2025 Shape Bias Project • MIT License
      </footer>

    </main>
  </div>
  <!-- ==========  /PAGE WRAPPER  ========== -->
</body>
</html>
